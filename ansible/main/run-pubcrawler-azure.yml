# This will provision an Azure instance and use it to run pubcrawler.
# A CosmoDB specified by cosmodb_mongo_url in the secrets file is used to
# store the data.
#
# Usage:
# pip install ansible[azure] azure azure-cli
# ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook run-pubcrawler.yml

- hosts: localhost
  tasks:
    - name: Create virtual machine
      azure_rm_virtualmachine:
        name: pubcrawlervm
        vm_size: Standard_D4
        admin_username: ubuntu
        ssh_password_enabled: false
        ssh_public_keys:
          - path: "/home/ubuntu/.ssh/authorized_keys"
            key_data: "{{ lookup('file', '~/.ssh/id_rsa.pub') }}"
        image:
          offer: UbuntuServer
          publisher: Canonical
          sku: '18.04-LTS'
          version: latest
        resource_group: pubcrawler
        virtual_network_name: pubcrawlernet
        subscription_id: "{{ azure_subscription_id }}"
        tenant: "{{ azure_tenant }}"
        client_id: "{{ azure_client_id }}"
        secret: "{{ azure_secret }}"
      register: azure
      tags: always
    
    - set_fact:
        azure_hostname: "{{ azure.ansible_facts.azure_vm.properties.networkProfile.networkInterfaces[0].properties.ipConfigurations[0].properties.publicIPAddress.properties.ipAddress }}"
      tags: always

    - name: Add all instance public IPs to host group
      add_host:
        hostname: "{{ azure_hostname }}"
        ansible_ssh_user: "ubuntu"
        ansible_python_interpreter: "/usr/bin/python3"
        groups:
          - azurehosts
      tags: always
    
    - name: Wait for the instances to boot by checking the ssh ports
      wait_for: host="{{ azure_hostname }}" port=22 delay=8 timeout=320 state=started
      tags: always

  vars_files:
    - my_secure.yml

- hosts: azurehosts
  vars:
    mongo_host: "{{ cosmodb_mongo_url }}"
  roles:
    - name: init
      become: yes
      become_method: sudo
      ansible_distribution_release: xenial
    - name: kamaln7.swapfile
      swapfile_size: 16GB
      swapfile_location: "/swapfile"
      become: yes
      become_method: sudo
      tags: swap
  tasks:
    - file: path="/pubcrawler" state=directory mode="a+rwx"
      become: yes
      become_user: root

    - name: Get pubcrawler code
      git:
        repo: "https://github.com/ecohealthalliance/pubcrawler.git"
        dest: "/pubcrawler"
        recursive: no
        force: yes

    - pip:
        virtualenv: /pubcrawler/venv
        virtualenv_python: python3
        requirements: /pubcrawler/requirements.txt

    - name: Install SpaCy model
      command: /pubcrawler/venv/bin/python -m spacy download en_core_web_md

    - name: Get EpiTator data
      get_url:
        url: https://s3.amazonaws.com/bsve-integration/annotator.sqlitedb
        dest: ~/.epitator.sqlitedb

    - when: restore_database_from_ftp | default(false)
      block:
      - name: Ensure /mnt is readable and writable
        file:
          path: /mnt
          state: directory
          mode: "a+rwx"
        become: yes
        become_method: sudo

      - name: Download pmc data
        command: wget -r -l1 ftp://ftp.ncbi.nlm.nih.gov/pub/pmc -A "*.tar.gz"
        args:
          chdir: /mnt
          creates: /mnt/ftp.ncbi.nlm.nih.gov

      - file:
          path: /mnt/ftp.ncbi.nlm.nih.gov/extracted
          state: directory

      - name: Extract pmc data
        shell: tar -xvzf /mnt/ftp.ncbi.nlm.nih.gov/pub/pmc/articles.*
        args:
          chdir: /mnt/ftp.ncbi.nlm.nih.gov/extracted

      - name: Import pmc data
        command: "/pubcrawler/venv/bin/python mongo_import_pmc_oas_local.py --pmc_path /mnt/ftp.ncbi.nlm.nih.gov/extracted"
        args:
          chdir: /pubcrawler

    - when: restore_database_from_dump | default(false)
      block:
      - name: Get PubMed data
        command: aws s3 cp s3://pubcrawler/data.zip /pubcrawler
        become: yes
        become_method: sudo
      - name: Install unzip
        apt: pkg=unzip state=installed
        become: yes
        become_method: sudo
  
      - name: Unzip PubMed data
        command: unzip -o /pubcrawler/data.zip
        args:
          chdir: /pubcrawler
          creates: /pubcrawler/dump
        ignore_errors: yes
        register: dump_dir
  
      - name: Restore PubMed db
        command: 'mongorestore --gzip --drop --uri="{{mongo_host}}"'
        args:
          chdir: /pubcrawler
        when: dump_dir.changed

    - name: Run keyword extraction script
      shell: '/pubcrawler/venv/bin/python crawler.py -u "{{mongo_host}}" -x extract_disease_ontology_keywords -s "keywords.disease-ontology" -w 8 -c articlesubset'
      args:
        chdir: /pubcrawler

    - name: Run geoname extraction script
      shell: '/pubcrawler/venv/bin/python crawler.py -u "{{mongo_host}}" -x extract_geonames -s keywords.geonames -w 8 -c articlesubset'
      args:
        chdir: /pubcrawler

  vars_files:
    - my_secure.yml

- hosts: localhost
  tasks:
    - name: Remove virtual machine
      azure_rm_virtualmachine:
        resource_group: pubcrawler
        name: pubcrawlervm
        state: absent
        subscription_id: "{{ azure_subscription_id }}"
        tenant: "{{ azure_tenant }}"
        client_id: "{{ azure_client_id }}"
        secret: "{{ azure_secret }}"

  vars_files:
    - my_secure.yml
